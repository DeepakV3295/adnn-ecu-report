\part{Implementation}

The traditional model for deploying neural network applications on embedded devices has over time developed the neural network inference step. The popular frameworks for machine learning such as PyTorch and Tensorflow provide approaches for porting neural network models written using those frameworks with a focus on allowing for model inference. Targetting even smaller devices with Tensorflow based neural network models is possible for inference only applications via Tensorflow Lite Micro \cite{tflm}. Efforts to allow training as well in these frameworks require more effort due to the compute and memory intensive nature of the training process.

This section contains the description of applications that train a neural network model to benchmark the training step on an embedded device. The neural network structure, the learning algorithm, and the dataset remain the same but the implementations are completed in traditional general purpose neural network frameworks as well as straightforward implementations in \texttt{C}, \texttt{C++}, and Python. The design and development of these application and an overview of the target hardware to perform the benchmarking are covered in the following chapters.

% ============================================
%        Design
% ============================================

\chapter{Design}

The benchmark applications test the training phase of a Handwritten Digit Recognition Neural Network (HDR-NN) on the MNIST \cite{mnist} dataset. MNIST is a popular dataset of handwritten digits commonly used for training image processing systems. It is a popular starting point for neural network implementations and has been used as the primary dataset in the benchmark experiments. The target embedded device is an Electronic Control Unit (ECU) with a Cortex-A9 processor.

\section{Neural Network Development Process}

The target environment necessitates the use of cross compilers and as part of the development process multiple build environments and systems were examined. Ultimately, the primary platform that ended up being used was the Yocto Project extensible SDK (eSDK) based application development process running on a standard linux based build environment. The QEMU emulator was also employed at various staged to check the build, and further test the application before moving onto tests on the actual hardware.

\subsection{Compiler Toolchains, Python pakcages, \& Yocto Recipes}

The \textit{meta-freescale} Yocto BSP layer by NXP supports the target processor and in combination with the Poky reference distribution provides an eSDK that was primarily used to test and develop the benchmark applications.

GCC based cross compilers and debuggers were usefull for the \texttt{C}, \texttt{C++}  programs. The \textit{meta-python} layer provided by Open Embedded was also useful in allowing for applications using Python and Numpy. The general portability of the benchmark applications and the Yocto project allows for further experiments to be conducted on different target architectures as well. For further optimisations that relies on hardware specific features such as ARM\textquotesingle s CMSIS-NN cannot be so easily ported however

\section[Handwritten Digit Recognition (HDR)]{HDR-NN Benchmark Programs}

The handwritten digit recognition neural network is a fully connected neural network and derives from the popular neural network textbook \href{http://neuralnetworksanddeeplearning.com}{neuralnetworksanddeeplearning.com}

The input layer has 784 neurons corresponding to 28 x 28 pixel images of the MNIST dataset and the output layer has 10 neurons corresponds to 10 different possible digits. The dimensions and depth of hidden layers of the network is configurable as well as other properties of the learning algorithm

\begin{center}
	\begin{tikzpicture}[x=2.4cm, y=1cm]
		\readlist\Shape{4,3,2,2}
		\readlist\Type{1,2,2,3}
		\readlist\Label{x,h^{(\prev)},h^{(\prev)},y}

		\def\yshift{0.5} % shift last node for dots

		\foreachitem \N \in \Shape{ % loop over layers
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\message{\lay,}
			\foreach \i [evaluate={
				\c=int(\i==\N);
				\y=\N/2-\i-\c*\yshift;
				\x=\lay; \n=\Type[\lay];
				}] in {1,...,\N}{
			\node[node \n] (N\lay-\i) at (\x,\y) {$\Label[\lay]$};

			\ifnum\lay>1
				\foreach \j in {1,...,\Shape[\prev]}{
				\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
				\draw[connect] (N\prev-\j) -- (N\lay-\i);
				}
			\fi
			}
			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.5] {$\vdots$};
		}
	\end{tikzpicture}
	\qquad
	\begin{tikzpicture}[x=2cm,y=1cm]
		\readlist\Shape{4,7,2}
		\readlist\Type{1,2,3}
		\readlist\Label{x,h^{(\prev)},y}

		\def\yshift{0.45}

		\foreachitem \N \in \Shape{
			\def\lay{\Ncnt}
			\pgfmathsetmacro\prev{int(\Ncnt-1)}

			\foreach \i [evaluate={
				\c=int(\i==\N);
				\y=\N/2-\i-\c*\yshift;
				\x=\lay; \n=\Type[\lay];
				}] in {1,...,\N}{

			\node[node \n] (N\lay-\i) at (\x,\y) {$\Label[\lay]$};

			\ifnum\lay>1
				\foreach \j in {1,...,\Shape[\prev]}{
				\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
				\draw[connect] (N\prev-\j) -- (N\lay-\i);
				}
			\fi
			}

			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.5] {$\vdots$};
		}
	\end{tikzpicture}
\end{center}

\subsection[HDR-NN Training]{The Learning Algorithm}

The HDR-NN benchmark applications will all share the same standard training algorithm listed below \ref{alg:cap}. Describing this algorithm in general purpose neural network frameworks is straight forward and plenty of general implementations of the algorithm exists, making the development process easier to target multiple programming paradigms. The configurable parameters of the learning algorithm in through out the implementations are the learning rate, the total number of epochs for training, and the batch size for gradient descent iterations.

\begin{algorithm}
	\caption{Mini Batch Gradient Descent with learning rate $\gamma$ and the Mean Squared Error (\texttt{MSE}) cost function}\label{alg:cap}
	\begin{algorithmic}
	\Require initial weights $w^{(0)}$, number of epochs $E$, batch size $B$, training data with $T$ entries
	\Ensure final weights $w^{(E*T)}$
	\For{$e = 0 \rightarrow E - 1$}
		\For{$b = 0 \rightarrow T / B$}
			\For{$t = b * B \rightarrow (b+1) * B$}
			\State estimate $\nabla \mathcal{L}(w^{(t)})$ \Comment{$\mathcal{L}$ here is \texttt{MSE}}
			\State compute $\Delta w^{(b)} += - \nabla \mathcal{L}(w^{(t)})$\label{lin:deep-learning-delta-w}
			\EndFor
			\State $w^{(e + 1)} := w^{(e)} + \gamma \Delta w^{(e)}$
		\EndFor
	\EndFor
	\State return $w^{(T)}$
	\end{algorithmic}
\end{algorithm}

\subsection{Training Configurations}

The model structure can be configured in the same manner across the implementations, as well as the learning algorithm configuration. This means that the shape of the model, the input parameters, the connections between the neuron can be configured in the same manner across the implementations. Furthermore, the learning rate, the number of epochs, and the batch size are also configurable in the same manner. Once the different implementations are configured in a similar manner, the training of the model is completed and the resulting weights are compared.

% ============================================
%        Development
% ============================================

\chapter{Development}

The HDR-NN benchmark application was completed in different programming languages and in neural network frameworks like PyTorch. Details about the target environment and the benchmark implementations are layed out in this chapter

\section[iMX6 Custom Board Target]{Target Hardware}

Exploring the target ECU board involved several examinations of a known state of the board. The linux kernel binaries were made via the Yocto project however there was no access to source code such as the recipes or the meta-layers themselves

The i.MX SoCs have a special boot mode named Serial Download Mode (SDM) typically accessible through boot switches. When configured into this mode, the ROM code will poll for a connection on a USB OTG port

\subsection[ECU / iMX6 Evaluation Board Overview]{i.MX6 Overview}

The iMX6 series is designed for high performance low power applications and target boards are configured with a single Cortex A9 core with the ARMv7 ISA. The processor supports NEON single-instruction multiple-data (SIMD) instructions, allowing for SIMD vector operations within the training program

\section{HDR-NN Implementation}

With the primary focus on training, MNIST dataset was primarily loaded in an easily readable format appropriate to the corresponding paradigms and the correctness verification routines and execution statistics measurement runs were seperated. The benchmark executions did not produce disk I/O after the dataset was read, unlike the correctness verification runs which produced the final weights from the execution runs that were subsequently compared with the other benchmark program execution output weights

\subsection[Python - Numpy]{The Reference HDR-NN in Python}

This is the baseline implementation and follows close to the implementation exhibitied on \href{http://neuralnetworksanddeeplearning.com}{neuralnetworksanddeeplearning.com}. The implementation uses the n-dimensional array data structure present in the popular Python programming language library Numpy

\subsection[Tensorflow Lite]{Tensorflow Lite based HDR-NN}

Developing ANNs on tensorflow using Keras is straightforward with good support and well documented APIs. Building the same model for a Tensorflow Lite (TFLite) was more involved however still straightforward

\subsection[C]{C based HDR-NN}

The \texttt{C} implementation had the least amount of external dependencies and contained the network in float arrays within structs.

\subsection[CPP - Eigen]{CPP based HDR-NN}

The CPP implementation used the n-dimensional array data structure feature of Eigen
