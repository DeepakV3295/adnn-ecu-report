\part{Implementation}

ANN training presents an important gap in the current efforts in Tiny ML. This section contains the description of benchmark ANN training applications created to test the performance of an ANN training cycle on an embedded board. The neural network structure, learning algorithm, and the dataset remain the same but the implementations are completed in traditional general purpose neural network frameworks as well as straightforward implementations in C and other languages

\chapter{Design}

The benchmark applications test the training phase of a Handwritten Digit Recognition Neural Network (HDRNN) on the MNIST dataset. MNIST is a popular database of handwritten digits commonly used for training image processing systems. It is a popular starting point for neural network implementations and has been used as the primary dataset in the benchmark experiments. The target embedded device is an Electronic Control Unit (ECU) board based on an iMX6 series processor

\section[Artificial Neural Network Development Process]{ANN Development Process}

The target environment necessitates the use of cross compilers and as part of the development process multiple build environments and systems were examined. Ultimately, the primary platform that ended up being used was the Yocto Project extensible SDK (eSDK) based application development process running on a standard linux based build environment

\subsection[Cross compilers \& Build System]{Compiler Toolchains \& Yocto Recipes}

The \textit{meta-freescale} Yocto BSP layer by NXP supports the target processor and in combination with Poky can provide an eSDK that was primarily used to test and develop the benchmark applications.

GCC based cross compilers and debuggers were usefull for the C, C++ programs. The general portability of the benchmark applications and the Yocto project allows for further experiments to be conducted on different target architectures as well. For further optimisations that relies on hardware specific features such as ARM\textquotesingle s CMSIS-NN cannot be so easily ported however

\section[Handwritten Digit Recognition (HDR)]{Benchmark ANN - HDRNN}

The handwritten digit recognition neural network is a fully connected neural network and derives from the popular neural network textbook \href{http://neuralnetworksanddeeplearning.com}{neuralnetworksanddeeplearning.com}

\begin{center}

	\begin{tikzpicture}[x=2.4cm,y=1.2cm]
		\readlist\Nnod{4,3,2} % array of number of nodes per layer
		\readlist\Nstr{1,2,3} % array of string number of nodes per layer
		\readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
		\def\yshift{0.45} % shift last node for dots

		\foreachitem \N \in \Nnod{ % loop over layers
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\message{\lay,}
			\foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
						\x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
			% NODES
			\node[node \n] (N\lay-\i) at (\x,\y) {$\Cstr[\lay]$};

			% CONNECTIONS
			\ifnum\lay>1 % connect to previous layer
				\foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
				\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
				\draw[connect] (N\prev-\j) -- (N\lay-\i);
				}
			\fi % else: nothing to connect first layer

			}
			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.5] {$\vdots$};
		}

		% LABELS
		\node[above=3,align=center,OliveGreen!60!black] at (N1-1.90)
			{input\\[-0.2em]layer};
		\node[above=2,align=center,RoyalPurple!60!black] at (N2-1.90)
			{hidden layers};
		\node[above=3,align=center,BrickRed!60!black] at (N\Nnodlen-1.90)
			{output\\[-0.2em]layer};

	\end{tikzpicture}

\end{center}

The input layer has 784 neurons corresponding to 28 x 28 pixel images of the MNIST dataset and the output layer has 10 neurons corresponds to 10 different possible digits. The dimensions and depth of hidden layers of the network is configurable as well as other properties of the learning algorithm

\subsection[HDR-NN Training]{The Learning Algorithm}

The HDRNN benchmark applications will all share the same standard training algorithm, namely Backpropagation with Stochastic Gradient Descent. Describing this algorithm in general purpose neural network frameworks is straight forward and plenty of general implementations of the algorithm exists in the wild, making the development process easier to target multiple programming paradigms. The configurable parameters of the learning algorithm in through out the implementations are the learning rate, the total number of epochs for training, and the batch size for gradient descent iterations

\subsection[Verifying Correctness]{Verifying Correctness}

The model structure can be configured in the same manner across the implementations, as well as the learning algorithm configuration. To verify their mutual correctness, all the implementations initialize HDRNN using the same PRNG

\chapter{Development}

The HDRNN benchmark application were completed in different programming languages and in neural network frameworks like Tensorflow. Details about the target environment and the benchmark implementations are layed out in this chapter

\section[iMX6 Custom Board Target]{Targetting an i.MX6 based custom board}

Exploring the target ECU board involved several examinations of a known state of the board. The linux kernel binaries were made via the Yocto project however there was no access to source code such as the recipes or the meta-layers themselves

The i.MX SoCs have a special boot mode named Serial Download Mode (SDM) typically accessible through boot switches. When configured into this mode, the ROM code will poll for a connection on a USB OTG port

\subsection[ECU / iMX6 Evaluation Board Overview]{i.MX6 Overview}

The iMX6 series is designed for high performance low power applications and target boards are configured with a single Cortex A9 core with the ARMv7 ISA. The processor supports NEON single-instruction multiple-data (SIMD) instructions, allowing for SIMD vector operations within the training program

\subsection{Testing on Device}

The benchmark tests were performed on ... using the perf tool

\section{HDRNN Implementation}

With the primary focus on training, MNIST dataset was primarily loaded in an easily readable format appropriate to the corresponding paradigms and the correctness verification routines and execution statistics measurement runs were seperated. The benchmark executions did not produce disk I/O after the dataset was read, unlike the correctness verification runs which produced the final weights from the execution runs that were subsequently compared with the other benchmark program execution output weights

\subsection[Python - Numpy]{The Reference HDR-NN in Python}

This is the baseline implementation and follows close to the implementation exhibitied on \href{http://neuralnetworksanddeeplearning.com}{neuralnetworksanddeeplearning.com}. The implementation uses the n-dimensional array data structure present in the popular Python programming language library Numpy

\subsection[Tensorflow Lite]{Tensorflow Lite based HDRNN}

Developing ANNs on tensorflow using Keras is straightforward with good support and well documented APIs. Building the same model for a Tensorflow Lite (TFLite) was more involved however still straightforward

\subsection[C]{C based HDRNN}

The C implementation had the least amount of external dependencies and contained the network in float arrays within structs.

\subsection[CPP - Eigen]{CPP based HDRNN}

\section{CMSIS-NN based Optimisations}
\textit{}

\section{General Distribution of Work}
