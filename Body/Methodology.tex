\part{Implementation}

ANN training presents an important gap in the current efforts in Tiny ML. This section contains the description of benchmark ANN training applications created to test the performance of an ANN training cycle on an embedded board. The neural network structure, learning algorithm, and the dataset remain the same but the implementations are completed in traditional general purpose neural network frameworks as well as straightforward implementations in C and other languages

\chapter{Design}

The benchmark applications test the training phase of a Handwritten Digit Recognition Neural Network (HDRNN) on the MNIST dataset. MNIST is a popular database of handwritten digits commonly used for training image processing systems. It is a popular starting point for neural network implementations and has been used as the primary dataset in the benchmark experiments. The target embedded device is an Electronic Control Unit (ECU) board based on an iMX6 series processor

\section[Artificial Neural Network Development Process]{ANN Development Process}

The target environment necessitates the use of cross compilers and as part of the development process multiple build environments and systems were examined - some of the details of this process is layed out in the following chapter. The primary platform used was the Yocto Project eSDK based application development process running on a standard linux based build environment

\subsection[Cross compilers \& Build System]{Compiler Toolchains \& Yocto Recipes}

The meta-freescale Yocto layer provides an SDK that was primarily used to test and develop the benchmark applications. GCC based cross compilers and debuggers were usefull for the C, C++ programs. The general portability of the benchmark applications and the Yocto project allows for further experiments to be conducted on different target architectures as well. For further optimisations that relies on hardware specific features such as ARM\textquotesingle s CMSIS-NN cannot be so easily ported however

\section[iMX6 Processor]{i.MX6 Processor}

The i.MX6 series of ARM processors has several variations out of which our selected target board contains Cortex A9 \dots


\subsection[ANN Acceleration]{i.MX6 as an ANN application target}
\textit{Describe performance optimisations possible using NEON(SIMD) \dots}

\subsection[CMSIS-NN]{Available Optimisation Frameworks : CMSIS-NN}
\textit{Describe ARM's CMSIS-NN as a framework worth adding}

\section[Handwritten Digit Recognition (HDR)]{Benchmark ANN - HDRNN}

The handwritten digit recognition neural network is a fully connected neural network and derives from the popular neural network textbook \href[]{http://neuralnetworksanddeeplearning.com}{neuralnetworksanddeeplearning.com}

\begin{center}

	\begin{tikzpicture}[x=2.4cm,y=1.2cm]
		\readlist\Nnod{4,3,2} % array of number of nodes per layer
		\readlist\Nstr{1,2,3} % array of string number of nodes per layer
		\readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
		\def\yshift{0.45} % shift last node for dots

		\foreachitem \N \in \Nnod{ % loop over layers
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\message{\lay,}
			\foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
						\x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
			% NODES
			\node[node \n] (N\lay-\i) at (\x,\y) {$\Cstr[\lay]$};

			% CONNECTIONS
			\ifnum\lay>1 % connect to previous layer
				\foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
				\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
				\draw[connect] (N\prev-\j) -- (N\lay-\i);
				}
			\fi % else: nothing to connect first layer

			}
			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.5] {$\vdots$};
		}

		% LABELS
		\node[above=3,align=center,OliveGreen!60!black] at (N1-1.90)
			{input\\[-0.2em]layer};
		\node[above=2,align=center,RoyalPurple!60!black] at (N2-1.90)
			{hidden layers};
		\node[above=3,align=center,BrickRed!60!black] at (N\Nnodlen-1.90)
			{output\\[-0.2em]layer};

	\end{tikzpicture}

\end{center}

The input layer has 784 neurons corresponding to 28 x 28 pixel images of the MNIST dataset and the output layer has 10 neurons corresponds to 10 different possible digits. The dimensions and depth of hidden layers of the network is configurable as well as other properties of the learning algorithm

\subsection[HDR-NN Training]{The Learning Algorithm}

The HDRNN benchmark applications will all share the same standard training algorithm, namely Backpropagation with Stochastic Gradient Descent. Describing this algorithm in general purpose neural network frameworks is straight forward and plenty of general implementations of the algorithm exists in the wild, making the development process easier to target multiple programming paradigms. The configurable parameters of the learning algorithm in through out the implementations are the learning rate, the total number of epochs for training, and the batch size for gradient descent iterations

\chapter{Development}

\begin{itemize}
	\item \textit{Detail the exploration on i.MX6 based board}
	\item \textit{Describe the HDRNN implementations}
\end{itemize}

\section[iMX6 Custom Board Target]{Targetting an i.MX6 based custom board}

Exploring the target ECU board involved several examinations of known state of the board. The linux kernel binaries were made via the Yocto project however there was no access to source code such as the recipes or the meta-layers themselves

\subsection{Overview of the Board H/W}
The board is designed for high performance low power applications and is configured with a single Cortex A9 core. This core utilises the ARMv7 architecture and features an advanced floating point unit (FPU) for efficient computation and storage of floating point numbers. Moreover, it supports the full range of single-instruction multiple-data (SIMD) instructions, allowing for SIMD vector operation to be executed quickly. Additionally, Cortex A9 includes NEON Technology, which is an advanced media processing engine for applications such as video and image encoding and decoding. This allows for efficient processing of multimedia data and multimedia applications to be implemented with minimal performance degradation

\subsection{Testing on Device}
\textit{Process for testing on device - flashing, benchmarking (perf)}

\section{HDRNN Implementation}
\textit{Motivate why HDRNN via MNIST dataset was chosen}

With the primary focus on training, MNIST dataset was primarily loaded in an easily readable format appropriate to the corresponding paradigms

\subsection[Python - Numpy]{Python Numpy based HDRNN}

\subsection[Tensorflow Lite]{Tensorflow Lite based HDRNN}

Developing ANNs on tensorflow using Keras is straightforward with good support and well documented APIs. Building the same model for a Tensorflow Lite (TFLite) was more involved however still straightforward

\subsection[C]{C based HDRNN}

\subsection[CPP - Eigen]{CPP based HDRNN}

\section{CMSIS-NN based Optimisations}
\textit{}

\section{General Distribution of Work}
